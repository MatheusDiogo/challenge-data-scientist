{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46522 entries, 0 to 46521\n",
      "Columns: 203 entries, index to REGIAO\n",
      "dtypes: float64(52), int64(103), object(48)\n",
      "memory usage: 72.1+ MB\n",
      "Das 203 colunas, 80 tem valores ausentes: ['col_51', 'col_52', 'col_53', 'col_54', 'col_55', 'col_56', 'col_57', 'col_58', 'col_59', 'col_60', 'col_61', 'col_62', 'col_63', 'col_64', 'col_65', 'col_66', 'col_67', 'col_68', 'col_69', 'col_70', 'col_71', 'col_72', 'col_73', 'col_74', 'col_75', 'col_76', 'col_77', 'col_78', 'col_79', 'col_80', 'col_81', 'col_82', 'col_83', 'col_84', 'col_85', 'col_86', 'col_109', 'col_110', 'col_111', 'col_112', 'col_113', 'col_114', 'col_115', 'col_116', 'col_117', 'col_118', 'col_119', 'col_120', 'col_121', 'col_122', 'col_138', 'col_139', 'col_140', 'col_141', 'col_142', 'col_150', 'col_151', 'col_154', 'col_156', 'col_158', 'col_160', 'col_167', 'col_168', 'col_169', 'col_171', 'col_172', 'col_174', 'col_178', 'col_180', 'col_181', 'col_184', 'col_185', 'col_186', 'col_194', 'col_196', 'col_197', 'col_198', 'col_199', 'col_200', 'REGIAO']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ler o arquivo CSV com separador de tabulação\n",
    "data = pd.read_csv('../datasets/credit_02/base_case.csv', sep='\\t')\n",
    "\n",
    "data.info()\n",
    "\n",
    "# Identificar colunas que contêm valores NaN\n",
    "colunas_com_nan = data.columns[data.isna().any()].tolist()\n",
    "\n",
    "# Exibir as colunas que contêm valores NaN\n",
    "print(f\"Das {len(data.columns)} colunas, {len(colunas_com_nan)} tem valores ausentes: {colunas_com_nan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informações de valores NaN por coluna:\n",
      "         Contagem NaN  Percentual NaN (%)\n",
      "col_181         44886           96.483384\n",
      "col_180         44886           96.483384\n",
      "col_160         43220           92.902283\n",
      "col_142         35694           76.724990\n",
      "col_116         35002           75.237522\n",
      "...               ...                 ...\n",
      "col_196          1368            2.940544\n",
      "col_167           543            1.167190\n",
      "col_174           543            1.167190\n",
      "col_184            98            0.210653\n",
      "col_185            95            0.204204\n",
      "\n",
      "[80 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Contar valores NaN em cada coluna\n",
    "contagem_nan_por_coluna = data.isna().sum()\n",
    "\n",
    "# Calcular a porcentagem de valores NaN em cada coluna\n",
    "percentual_nan_por_coluna = (contagem_nan_por_coluna / data.shape[0]) * 100\n",
    "\n",
    "# Combinar as duas informações em um DataFrame para exibição\n",
    "nan_info = pd.DataFrame({\n",
    "    'Contagem NaN': contagem_nan_por_coluna,\n",
    "    'Percentual NaN (%)': percentual_nan_por_coluna\n",
    "})\n",
    "\n",
    "# Filtrar apenas as colunas que têm valores NaN e ordenar pelo percentual\n",
    "nan_info_filtrado = nan_info[nan_info['Contagem NaN'] > 0].sort_values(by='Percentual NaN (%)', ascending=False)\n",
    "\n",
    "# Exibir as informações de valores NaN\n",
    "print(\"Informações de valores NaN por coluna:\")\n",
    "print(nan_info_filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas com mais de 20% de valores ausentes: ['col_181', 'col_180', 'col_160', 'col_142', 'col_116', 'col_151', 'col_121', 'col_122', 'col_120', 'col_113', 'col_114', 'col_186', 'col_140', 'col_156', 'col_158', 'col_138', 'col_139', 'col_171', 'col_172', 'col_154', 'col_150', 'col_168', 'col_141', 'col_115']\n",
      "Colunas removidas: 24\n"
     ]
    }
   ],
   "source": [
    "# Definir o limite de porcentagem para remover colunas\n",
    "limite_percentual = 20\n",
    "\n",
    "# Selecionar colunas para remoção\n",
    "colunas_acima_20_nan = nan_info_filtrado[nan_info_filtrado['Percentual NaN (%)'] >= limite_percentual].index\n",
    "\n",
    "print(\"Colunas com mais de 20% de valores ausentes:\", colunas_acima_20_nan.to_list())\n",
    "\n",
    "# Remover as colunas do DataFrame original\n",
    "data = data.drop(columns=colunas_acima_20_nan)\n",
    "\n",
    "print(f\"Colunas removidas: {len(colunas_acima_20_nan)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas com um único valor: ['PRODUTO', 'col_5', 'col_8', 'col_12', 'col_17', 'col_21', 'col_33', 'col_34', 'col_35', 'col_36', 'col_37', 'col_38', 'col_39', 'col_40', 'col_41', 'col_42', 'col_43', 'col_44', 'col_45', 'col_46', 'col_47', 'col_71', 'col_72', 'col_73', 'col_74', 'col_75', 'col_76', 'col_90', 'col_96', 'col_103', 'col_104']\n",
      "Colunas removidas: 31\n"
     ]
    }
   ],
   "source": [
    "# Indentificar Colunas com um único valor\n",
    "colunas_valores_unicos = data.columns[data.nunique() == 1].to_list()\n",
    "\n",
    "# Exibindo Colunas com um único valor\n",
    "print(f'Colunas com um único valor: {colunas_valores_unicos}')\n",
    "\n",
    "# Removendo Colunas com um único valor do DataFrame\n",
    "data = data.drop(columns = colunas_valores_unicos)\n",
    "\n",
    "print(f\"Colunas removidas: {len(colunas_valores_unicos)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas duplicadas: []\n"
     ]
    }
   ],
   "source": [
    "# Identificar colunas duplicadas\n",
    "colunas_duplicadas = data.columns[data.T.duplicated(keep=False)].tolist()\n",
    "\n",
    "# Exibir as colunas duplicadas\n",
    "print(\"Colunas duplicadas:\", colunas_duplicadas)\n",
    "\n",
    "# Primeiramente fiz esse código! Porém, percebi que as duplicadas so continham o valor \"0\".\n",
    "# Por isso, removi todas com um único valor.\n",
    "\n",
    "# # Exibir as colunas duplicadas para inspeção\n",
    "# if colunas_duplicadas:\n",
    "#     print(\"\\nExemplos de colunas duplicadas:\")\n",
    "#     for coluna in colunas_duplicadas:\n",
    "#         print(f\"\\nColuna: {coluna}\")\n",
    "#         print(data[coluna].head())\n",
    "\n",
    "# # Remover colunas duplicadas do DataFrame\n",
    "# data = data.drop(columns = colunas_duplicadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratando coluna de Data\n",
    "# Adicionando coluna de dia\n",
    "data[\"dia\"] = data[\"REF_DATE\"].apply(lambda x: int(str(x).split('-')[2]))\n",
    "\n",
    "# Adicionando coluna mês\n",
    "data[\"mes\"] = data[\"REF_DATE\"].apply(lambda x: int(str(x).split('-')[1]))\n",
    "\n",
    "# Adicionando coluna mês\n",
    "data[\"ano\"] = data[\"REF_DATE\"].apply(lambda x: int(str(x).split('-')[0]))\n",
    "\n",
    "# Removendo coluna Mes de Referencia e index\n",
    "data = data.drop(columns = [\"REF_DATE\", 'index'])\n",
    "\n",
    "# Criando um Arquivo para fazer análises dos dados\n",
    "data.to_excel('Dados.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando colunas categoricas em colunas numericas\n",
    "# Colunas para One-Hot Encoding\n",
    "colunas_one_hot = ['col_162', 'col_163']\n",
    "\n",
    "# Aplicar One-Hot Encoding nas colunas\n",
    "data = pd.get_dummies(data, columns=colunas_one_hot, drop_first=True)\n",
    "\n",
    "# Identificar outras colunas categóricas\n",
    "colunas_categoricas = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Aplicar Label Encoding nas outras colunas categóricas\n",
    "label_encoders = {}\n",
    "for col in colunas_categoricas:\n",
    "    if col not in colunas_one_hot:\n",
    "        le = LabelEncoder()\n",
    "        data[col] = le.fit_transform(data[col].astype(str))\n",
    "        label_encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preencher valores NaN com a mediana de cada coluna\n",
    "data = data.apply(lambda x: x.fillna(x.median()), axis=0)\n",
    "\n",
    "# Temos uma quantidade boa de dados e poderiamos remover as instancias com dados ausentes.\n",
    "# Porém, se um contexto maior das caracteristicas, preferi preencher com a mediana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o DataFrame resultante em um novo arquivo Excel\n",
    "data.to_excel('Dados Tratados.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
